{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries \n",
    "\n",
    "As a data engineer, you would typically use libraries such as: \n",
    "- `pandas` : used to read data into a structured tabular format known as a `DataFrame`. It supports reading data from files, databases and APIs. It allows for operations to be performed on the `DataFrame` before then being written out to another file or database. \n",
    "- `requests` : used to send complex queries to APIs to fetch data.  \n",
    "- And much more..! Depending on what you need to do. \n",
    "\n",
    "Go ahead an import these popular libraries into your notebook by running\n",
    "\n",
    "```python\n",
    "import pandas as pd  # it is common to provide an alias (shortened name) for the library you're importing to make it easier to reference in your code \n",
    "```\n",
    "\n",
    "If these libraries do not exist on your computer, you would see a `Module Not Found` error. In that case, go ahead and install these libraries by running: \n",
    "\n",
    "```\n",
    "pip install pandas \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting data \n",
    "\n",
    "As a data engineer, the types of data that you are ingesting will depend on the organisation or company you work for. For example: \n",
    "- Finance data\n",
    "- Operational data \n",
    "- Market research data\n",
    "- and many more! \n",
    "\n",
    "The data can exist in a myraid of different formats, for example: \n",
    "- comma seperated values (csv) files \n",
    "- excel files \n",
    "- parquet files \n",
    "- JSON files \n",
    "- REST APIs (JSON)\n",
    "- SOAP APIs (XML)\n",
    "- database tables (SQL) \n",
    "- kafka streams \n",
    "- and so much more..! \n",
    "\n",
    "Therefore, as a data engineer, you will need to learn how to ingest data from these disparate sources. To make our lives easier, there are really 4 main data formats that we can think about when ingesting data: \n",
    "1. database tables \n",
    "2. Web (REST API/JSON)\n",
    "3. files (csv, excel, json, parquet, etc)\n",
    "4. streaming data (kafka streams)\n",
    "\n",
    "If you are able to master each one, then you are going to be valuable in the eyes of your company. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's ingest\n",
    "\n",
    "For this instructor demo, we will be ingesting data from the following sources: \n",
    "\n",
    "1. Customers dataset: `olist_customers_dataset.csv`\n",
    "2. Orders dataset: `olist_orders_dataset.csv` \n",
    "3. Order items dataset: `olist_order_items_dataset.csv` \n",
    "4. Products dataset: `olist_products_dataset.csv` \n",
    "5. Product category translation: `product_category_name_translation.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv files into their respective DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first 5 rows in each DataFrame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first 5 rows in each DataFrame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first 5 rows in each DataFrame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first 5 rows in each DataFrame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first 5 rows in each DataFrame \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! Now that we have ingested the CSV files into DataFrames, what do we do next? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming data \n",
    "\n",
    "Once we have ingested the raw data, the next step is to enrich the data so that it can be used by **Data Analysts** to answer business questions.\n",
    "\n",
    "*How do we enrich the data?*\n",
    "\n",
    "The step of enriching the data is known as `Transformation`. Transformation typically involves steps like: \n",
    "- removing missing records from the data \n",
    "- renaming columns and values to make the data consistent and easier to use \n",
    "- joining a dataset with another to make more data available to the Data Analyst \n",
    "- creating new columns which are calculations performed on previous ones \n",
    "- aggregating the data \n",
    "\n",
    "*What do we use to perform transformations?*\n",
    "\n",
    "As a data engineer, you would typically perform transformations using the following tools: \n",
    "\n",
    "- Database SQL: first load your data into a database table. Then perform the transformation using `Structured Query Language (SQL)`. \n",
    "- Pandas DataFrame: first load your data into a Pandas DataFrame. Then perform the transformation using pandas and/or python functions. \n",
    "- Spark DataFrame: Spark is a distributed computing framework that allows you to perform operations on very large datasets using many computers. First load your data into a Spark DataFrame. Then perform the transformation using spark and/or user defined functions.  \n",
    "\n",
    "For our exercise today, we will be using a Pandas DataFrame. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do I transform? \n",
    "\n",
    "Deciding how to transform is where you will have to use your analytical skills, and also engage the Data Analyst or the end-user to determine what shape the data needs to be in order to answer business questions. For example, a business question could be: \n",
    "- \"What are my most popular products?\"\n",
    "- \"Which countries generate the highest revenue?\"\n",
    "\n",
    "For our exercise, we will refer to the database diagram below (also known as a database schema). The database diagram will tell us how the DataFrames relate to one another and therefore, how they should be combined together to answer business questions. \n",
    "\n",
    "<img src=\"../resources/ecommerce_database_schema.png\" alt=\"schema\" style=\"width:600px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge customers with orders and save result to a new DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge customers and orders with order items and save result to a new DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge customers, orders and order items with products and save result to a new DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge merge customers, orders, order items, and products with product category translation and save result to a new DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all the columns in our final DataFrame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the following columns: [\"customer_id\", \"customer_city\", \"customer_state\", \"order_purchase_timestamp\", \"price\", \"freight_value\", \"product_category_name\", \"product_name_lenght\", \"product_description_lenght\", \"product_photos_qty\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column called \"total_value\" which is the sum of the price and freight_value \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! Your transformation steps are complete! The new DataFrame is ready to be saved to a CSV file for the Data Analyst to use for their data analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data \n",
    "\n",
    "After completing your transformation steps, you are ready to save your data so that others can consume the datasets you've produced. \n",
    "\n",
    "There are several locations you can save your data to: \n",
    "1. **Save dataset to a database table**: \n",
    "    - do this if the data analysts are only comfortable the SQL language. \n",
    "    - depending on the size of your dataset, you may choose to go with a standard SQL database like MySQL, PostgreSQL, Microsoft SQL Server. If your dataset is large enough, then you would have to consider Massively Parallel Processing databases like: AWS Redshift, Snowflake, Google BigQuery, or Azure Synapse. \n",
    "\n",
    "\n",
    "2. **Save dataset to a file**: \n",
    "    - do this if the data analysts are comfortable with accessing data with multiple languages like: SQL, Python, R, Scala. \n",
    "    - choose a file format that maximises: storage size (compression), query performance (how quickly the computer is able to read the file), formats that users are familiar with. For example: parquet, avro, csv, delta lake. \n",
    "    - to make the data accessible to data analysts over the cloud, you will likely choose to store your data in a file storage system for example: AWS S3 buckets, Azure Data Lake, GCP Cloud Storage. \n",
    "    - files are now becoming more popular as the choice to store data because of new architectural paradigms such as the data lakehouse. \n",
    "\n",
    "To save your data when using Pandas, simply perform: \n",
    "\n",
    "```python\n",
    "df.to_csv(\"your file path here\", index=False) # index=False to remove the index column in the DataFrame when saving \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "737dee95f12251952d689b173c016b638423f6644d23ceedebaa683681f8eb4f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('data101': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
